{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The Wikifier consists of stages as follows: </b>\n",
    "1. Anchor generation\n",
    "2. Bipartite Graph edges generation\n",
    "3. Concept-Concept edges generation\n",
    "4. Pagerank generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Import Libraries </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:27:13.061653Z",
     "start_time": "2020-04-20T18:27:10.008805Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from csv import reader, writer\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "stopword_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Indicate the Input-Output directory paths </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:27:13.072283Z",
     "start_time": "2020-04-20T18:27:13.063146Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "FILE_EXTENSION = \".csv\"\n",
    "\n",
    "CORPUS_PATH = \"../corpus/\"\n",
    "CORPUS_DELIMITER = \",\"\n",
    "\n",
    "corpus_files = dict()\n",
    "corpus_file_list = [\"anchor-id\",\"concept-id\", \"concept-concepts\", \"anchor-concepts\", \"anchor-entropy\"]\n",
    "for filename in corpus_file_list:\n",
    "    corpus_files[filename] = CORPUS_PATH + filename + FILE_EXTENSION\n",
    "    \n",
    "input_path = \"../input/\"\n",
    "input_file = input_path + \"input.txt\"\n",
    "\n",
    "WIKI_PATH = \"../wiki_temp/\"\n",
    "if not os.path.exists(WIKI_PATH):\n",
    "    os.makedirs(WIKI_PATH)\n",
    "    \n",
    "wiki_files = dict()\n",
    "wiki_file_list = [ \"wiki_anchors\",\"wiki_bipartite_edges\",\"wiki_concept_edges\", \"wiki_output\", \"wiki_pagerank\" ]\n",
    "\n",
    "for filename in wiki_file_list:\n",
    "    wiki_files[filename] = WIKI_PATH + filename + FILE_EXTENSION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to generate Anchors\n",
    "\n",
    "#### Using NGRAM Words for Anchors\n",
    "- After cleaning the input, generate all possible n-grams.\n",
    "- For each n-gram generated from the input, check if it is an anchor and store it in a dictionary along with its start and end position.\n",
    "- Construct a inverted anchor dictionary with (start,end) as key and anchor string as value. \n",
    "- Eliminate all sub-mention anchors from the inverted anchor dictionary and return the dictionary.\n",
    "\n",
    "#### Using Noun phrases for Anchors\n",
    "\n",
    "- Instead of generating all possible candidates from a given text, we use the noun phrases in a text as Anchors\n",
    "- We use Spacy tool to extract Noun phrases from a text and use them directly as anchors\n",
    "- Punctuation are not removed from the text\n",
    "\n",
    "### Reducing Ambiguous Anchors\n",
    "\n",
    "**Entropy of the mention** is the amount of uncertainty regarding the link target given the fact that its anchor text.\n",
    "\n",
    "   **Entropy** `H(a) = – sum ([(ni/n) * log(ni/n)])`\n",
    "\n",
    "If this entropy is above a user-specified threshold (e.g. 3 bits), we completely ignore the mention as being too ambiguous to be of any use. For mentions that pass this heuristic, we sort the target pages in decreasing order of `ni` and use only the top few of them (e.g. top 20) as candidates in our mention-concept graph.\n",
    "\n",
    "### Advanced Anchor filtering\n",
    "\n",
    "- Ignore candidates for which `ni` itself is below a certain threshold (e.g. `ni` < 2), the idea being that if such a phrase occurs only once as the anchor text of a link pointing to that candidate, this may well turn out to be noise and is best disregarded.\n",
    "\n",
    "- The Wikifier can also be configured to ignore certain types of concepts based on their Wikidata class membership. This can be useful to exclude from consideration Wikipedia pages that do not really correspond to what is usually thought of as entities (e.g. “List of…” pages).\n",
    "\n",
    "- Ignore any mention that consists entirely of stopwords and/or very common words (top 200 most frequent words in the Wikipedia for that particular language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorExtractor():\n",
    "\n",
    "    def __init__(self, anchor_id_file):\n",
    "        self.anchors = list()\n",
    "        self.elastic_index = self.get_anchor_id(anchor_id_file)\n",
    "        \n",
    "    def get_anchor_id(self, anchor_id_file):\n",
    "        result = dict()\n",
    "        with open(anchor_id_file, \"r\") as fileptr:\n",
    "            temp_list = fileptr.readlines()\n",
    "            for line in temp_list:\n",
    "                temp = line.strip().split(CORPUS_DELIMITER, 1)\n",
    "                if len(temp) > 1:\n",
    "                    result[temp[1]] = temp[0]\n",
    "        return result\n",
    "    \n",
    "    def word_to_id(self, anchors):\n",
    "        return [ self.elastic_index[phrase] for phrase in anchors if self.is_in_elastic_index(phrase)]\n",
    "    \n",
    "    def set_anchors(self):\n",
    "        self.anchors = self.word_to_id(set(self.anchors))\n",
    "        \n",
    "    def get_anchors(self):\n",
    "        return self.anchors\n",
    "    \n",
    "    def filter_stopwords(self):\n",
    "        temp = [ phrase for phrase in self.anchors if phrase.lower() not in stopword_set and not phrase.isnumeric()]\n",
    "        result = list()\n",
    "        for phrase in temp:\n",
    "            filtered_phrase = self.phrase_filter(phrase)\n",
    "            if filtered_phrase != None:\n",
    "                result.append(filtered_phrase)\n",
    "        self.anchors = result\n",
    "     \n",
    "    #Entropy\n",
    "    def entropy_reader(self, entropy_file):\n",
    "        result = dict()\n",
    "        with open(entropy_file, newline='\\n') as csvfile:\n",
    "            red = reader(csvfile, delimiter= CORPUS_DELIMITER)\n",
    "            for row in red:\n",
    "                result[row[0]] = float(row[1])\n",
    "        return result\n",
    "    \n",
    "    def entropy_filter(self, entropy_tuple):\n",
    "        file, limit = entropy_tuple\n",
    "        entropy_dict = self.entropy_reader(file)\n",
    "        temp = self.get_anchors()\n",
    "        self.anchors = [anchor for anchor in temp if entropy_dict[anchor] < limit]\n",
    "    \n",
    "    #Noun Anchors\n",
    "    def is_in_elastic_index(self,string):\n",
    "        return string in self.elastic_index  \n",
    "    \n",
    "    def phrase_filter(self, phrase):\n",
    "        punct_set = set(string.punctuation)\n",
    "        list_of_words = phrase.split(\" \")\n",
    "        for i, val in enumerate(phrase.split()):\n",
    "            if val not in punct_set and val.lower() not in stopword_set:\n",
    "                if i==0:\n",
    "                    return phrase\n",
    "                return \" \".join(list_of_words[i:])\n",
    "            \n",
    "    def get_noun_anchors(self, input):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(input)\n",
    "        nouns = set()\n",
    "        for chunk in doc.noun_chunks:\n",
    "            phrase = self.phrase_filter(chunk.text.strip())\n",
    "            if phrase != None and not phrase.isnumeric():\n",
    "                nouns.add(phrase)\n",
    "        self.anchors = [anchor for anchor in nouns if self.is_in_elastic_index(anchor)]\n",
    "    \n",
    "    #N-gram Anchors\n",
    "    def clean(self, input):\n",
    "        punc_dict = dict.fromkeys(string.punctuation)\n",
    "        self.translator = str.maketrans(punc_dict)\n",
    "        for character in self.translator:\n",
    "            self.translator[character] = \" \"   \n",
    "        input = input.strip().translate(self.translator)\n",
    "        input = \" \".join(input.split())\n",
    "        return input\n",
    "\n",
    "    def generate_ngrams(self,string):\n",
    "        string_list = string.split()\n",
    "        candidate_grams = dict()\n",
    "        seen_grams = dict()\n",
    "        n = len(string_list)\n",
    "        for i in range(n):\n",
    "            for j in range(i+1,n+1):\n",
    "                new_string = \" \".join(string_list[i:j])\n",
    "                if new_string in seen_grams:\n",
    "                    if new_string in candidate_grams:\n",
    "                        candidate_grams[new_string].append((i,j-1))\n",
    "                else:\n",
    "                    seen_grams[new_string] = 1\n",
    "                    if self.is_in_elastic_index(new_string):\n",
    "                        #print(new_string)\n",
    "                        candidate_grams[new_string] = [(i,j-1)]\n",
    "\n",
    "        return candidate_grams,seen_grams\n",
    "\n",
    "    def generate_inverted_anchor_grams(self,candidate_grams):\n",
    "        index_grams = dict()\n",
    "        for key,values in candidate_grams.items():\n",
    "            for co_ordinates in values:\n",
    "                index_grams[co_ordinates] = key\n",
    "        return index_grams\n",
    "\n",
    "    def eliminate_sub_mentions(self, index_grams):\n",
    "        iterator = list(index_grams.keys())\n",
    "        iterator = sorted(iterator, key=lambda element: (element[1], -element[0]))\n",
    "        flag = False\n",
    "        n = len(iterator)\n",
    "        for i in range(n-1):\n",
    "            a,b = iterator[i]\n",
    "            c,d = iterator[i+1]\n",
    "            if a>=c and b<=d:\n",
    "                del index_grams[(a,b)]\n",
    "                flag = True\n",
    "        if flag:\n",
    "            return self.eliminate_sub_mentions(index_grams)\n",
    "        return index_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the Input file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_reader(filename):\n",
    "    input = \"\"\n",
    "    with open(filename, \"r\") as fileptr:\n",
    "        line_list = fileptr.readlines()\n",
    "        for line in line_list:\n",
    "            input += \" \" + line.strip()\n",
    "    return input\n",
    "\n",
    "def anchor_writer(data, file_name):\n",
    "    with open(file_name, 'w', newline='\\n') as write_obj:\n",
    "        csv_writer = writer(write_obj)\n",
    "        csv_writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_anchors(flags, anchor_id_file, output_file):\n",
    "    \n",
    "    anchor_extractor_object = AnchorExtractor(anchor_id_file)\n",
    "    input = input_reader(flags[\"input\"])\n",
    "    if flags[\"process\"] == \"ngrams\" or flags[\"process\"] == \"all\":\n",
    "        clean_input = anchor_extractor_object.clean(input)\n",
    "        \n",
    "        #generate candidate anchor grams\n",
    "        candidate_grams,seen_grams = anchor_extractor_object.generate_ngrams(clean_input)\n",
    "        \n",
    "        #Construct inverted index dictionary\n",
    "        index_grams = anchor_extractor_object.generate_inverted_anchor_grams(candidate_grams)\n",
    "        \n",
    "        #generate the mentions\n",
    "        anchor_grams = anchor_extractor_object.eliminate_sub_mentions(index_grams.copy())\n",
    "        anchor_extractor_object.anchors = [word for word in list(anchor_grams.values()) ]\n",
    "        anchor_extractor_object.filter_stopwords()\n",
    "        anchor_extractor_object.set_anchors()\n",
    "        ngram_anchors = set(anchor_extractor_object.get_anchors())\n",
    "        \n",
    "    if flags[\"process\"] == \"nouns\" or flags[\"process\"] == \"all\":\n",
    "        anchor_extractor_object.get_noun_anchors(input)\n",
    "        anchor_extractor_object.set_anchors()\n",
    "        noun_anchors = set(anchor_extractor_object.get_anchors())\n",
    "    \n",
    "    if flags[\"process\"] == \"all\":\n",
    "        anchor_extractor_object.anchors = list( ngram_anchors.intersection(noun_anchors) )\n",
    "            \n",
    "    if flags[\"entropy\"][1] > 0:\n",
    "        anchor_extractor_object.entropy_filter(flags[\"entropy\"])\n",
    "    \n",
    "    result_anchors = anchor_extractor_object.get_anchors()\n",
    "    anchor_writer(result_anchors, output_file)\n",
    "    return result_anchors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_limit = 3\n",
    "flags = dict()\n",
    "\n",
    "flags[\"input\"] = input_file\n",
    "\n",
    "# Allowed values nouns, ngrams, all\n",
    "flags[\"process\"] = \"nouns\"\n",
    "\n",
    "flags[\"entropy\"] = (corpus_files[\"anchor-entropy\"], entropy_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_anchors = extract_anchors(flags, corpus_files[\"anchor-id\"], wiki_files[\"wiki_anchors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a3397319',\n",
       " 'a2478803',\n",
       " 'a2478802',\n",
       " 'a34964',\n",
       " 'a411189',\n",
       " 'a91518',\n",
       " 'a377091',\n",
       " 'a6549',\n",
       " 'a7008',\n",
       " 'a8739039',\n",
       " 'a43525',\n",
       " 'a7459',\n",
       " 'a5458',\n",
       " 'a118521',\n",
       " 'a752362',\n",
       " 'a43653',\n",
       " 'a67731',\n",
       " 'a41907',\n",
       " 'a21',\n",
       " 'a3132']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipartite Edges Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each mention in the wiki_anchors, get all the corresponding wikipedia URI that it points to.\n",
    "2. For each mention, URI pair create an edge (m,c) this is stored in `anchor-concepts.csv`\n",
    "3. Assign the value for each edge as,\n",
    "\n",
    "        P(m→c) =  number of hyperlinks with m pointing to c\n",
    "        \t\t  __________________________________________\n",
    "        \t\t   \n",
    "                  number of hyperlinks with m as anchor text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Import Libraries </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:27:13.099155Z",
     "start_time": "2020-04-20T18:27:13.073966Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "from csv import writer\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipartiteEdgeGenerator:\n",
    "    \n",
    "    def __init__(self, wiki_anchors, anchor_concept_file):\n",
    "        anchors = wiki_anchors\n",
    "        anchor_concept_dict = self.get_anchor_concept_dict(anchors, anchor_concept_file)\n",
    "        self.bipartite_edges = self.get_bipartite_edges(anchor_concept_dict)\n",
    "    \n",
    "    def get_anchor_concept_dict(self, anchors, anchor_concept_file):\n",
    "        temp_dict = dict()\n",
    "        with open(anchor_concept_file, \"r\") as fileptr:\n",
    "            temp_list = fileptr.readlines()\n",
    "            for line in temp_list:\n",
    "                temp = line.strip().split(CORPUS_DELIMITER)\n",
    "                if temp[0] in anchors:\n",
    "                    if temp[0] in temp_dict:\n",
    "                        temp_dict[temp[0]].append(temp[1:])\n",
    "                    else:\n",
    "                        temp_dict[temp[0]] = [temp[1:]]\n",
    "        return temp_dict\n",
    "        \n",
    "    def get_bipartite_edges(self, anchor_concept_dict):\n",
    "        result = list()\n",
    "        for anchor in anchor_concept_dict:\n",
    "            edge_list = anchor_concept_dict[anchor]\n",
    "            result += [ [anchor] + edge for edge in edge_list ]\n",
    "        return result\n",
    "        \n",
    "    def output_writer(self, file_name):\n",
    "        with open(file_name, 'w', newline='\\n') as write_obj:\n",
    "            csv_writer = writer(write_obj)\n",
    "            for data in self.bipartite_edges:\n",
    "                csv_writer.writerow(data)\n",
    "        temp = [[a,b,float(c)] for a,b,c in self.bipartite_edges]\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_anchors(wiki_anchor_file):\n",
    "    anchors = list()\n",
    "    with open(wiki_anchor_file, \"r\") as fileptr:\n",
    "        temp_list = fileptr.readlines()\n",
    "        for line in temp_list:\n",
    "            anchors += line.strip().split(CORPUS_DELIMITER)\n",
    "    return set(anchors)\n",
    "try:\n",
    "    wiki_anchors\n",
    "except NameError:\n",
    "    wiki_anchors = get_wiki_anchors(wiki_files[\"wiki_anchors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite_object = BipartiteEdgeGenerator(wiki_anchors, corpus_files[\"anchor-concepts\"])\n",
    "wiki_bipartite_edges = bipartite_object.output_writer(wiki_files[\"wiki_bipartite_edges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a21', 'c22', 1.827084703646861e-05]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_bipartite_edges[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept-Concept Edges Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `Lc` is the set of wikipedia URI that points to c and N is the number of concepts in wikipedia then Semantic relatedness SR between concepts c and c' is\n",
    "\n",
    "   `SR(c, c′) = 1 − [(log max{|Lc |, |Lc′|} − log|Lc ∩ L c′ |) / (log N − log min{|Lc |, |Lc′ |)]`\n",
    "    \n",
    "\n",
    "1. For each wikipedia URI in the graph generated calculate the Semantic relatedness SR with every other wikipedia URI in the graph\n",
    "2. For each (c, c') in the graph, if `SR(c,c')` > 0, then create the edge `c→c'` and assign the value for edge as,\n",
    "\n",
    "    `P(c→c') = SR(c, c') / [For all c'' sum(SR(c, c''))]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Import Libraries </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:27:13.109694Z",
     "start_time": "2020-04-20T18:27:13.101776Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from numpy import log10 as log\n",
    "from csv import reader, writer\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptEdgeGenerator():\n",
    "\n",
    "    def __init__(self, cc_file, candidate_concepts):\n",
    "        self.cc_dict_file_name = cc_file\n",
    "        self.cc_edges = list()\n",
    "        self.concept_ids = candidate_concepts\n",
    "\n",
    "    def read_cc_file(self, filename):\n",
    "        cc_dict = dict()\n",
    "        with open(filename, newline='\\n') as csvfile:\n",
    "            red = reader(csvfile, delimiter=CORPUS_DELIMITER)\n",
    "            for s,t in red:\n",
    "                if s in cc_dict:\n",
    "                    cc_dict[s].add(t)\n",
    "                else:\n",
    "                    cc_dict[s] = set([t])\n",
    "        return cc_dict\n",
    "\n",
    "    def get_candidate_concept_ids(self, filename):\n",
    "        concept_ids = set()\n",
    "        with open(filename, newline='\\n') as csvfile:\n",
    "            red = reader(csvfile, delimiter=',')\n",
    "            for row in red:\n",
    "                concept_ids.add(row[1])\n",
    "        return concept_ids\n",
    "                \n",
    "#   Concept - Concept Edges\n",
    "    def augment_sr_to_concept(self, c1, c2, temp_concepts, SR):\n",
    "        temp_concepts[c1] = temp_concepts[c1]+SR if c1 in temp_concepts else SR\n",
    "        temp_concepts[c2] = temp_concepts[c1]+SR if c2 in temp_concepts else SR    \n",
    "        return temp_concepts\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        cc_lookup_dict = self.read_cc_file(self.cc_dict_file_name)\n",
    "        self.logN = log(len(cc_lookup_dict))\n",
    "        \n",
    "        predecessors = dict()\n",
    "        for cid in self.concept_ids:\n",
    "            if cid in cc_lookup_dict:\n",
    "                predecessors[cid] = cc_lookup_dict[cid]\n",
    "        return predecessors\n",
    "    \n",
    "    def get_concept_concept_edge_values(self):\n",
    "        temp_concepts = dict()\n",
    "        result_dict = dict()\n",
    "        \n",
    "        predecessors = self.get_predecessors()\n",
    "        \n",
    "        for c1, c2 in combinations(self.concept_ids, 2):\n",
    "            predecessors_c1 = predecessors[c1]\n",
    "            predecessors_c2 = predecessors[c2]\n",
    "            intersects = predecessors_c1.intersection(predecessors_c2)\n",
    "\n",
    "            if len(intersects) > 0:\n",
    "                SR = 1\n",
    "                len_pred_c1 = len(predecessors_c1)\n",
    "                len_pred_c2 = len(predecessors_c2)\n",
    "                max_len = max(len_pred_c1, len_pred_c2)\n",
    "                min_len = min(len_pred_c1,len_pred_c2)\n",
    "                SR -= (log(max_len) - log(len(intersects))) / (self.logN - log(min_len))\n",
    "                result_dict[(c1,c2)] = SR\n",
    "                temp_concepts = self.augment_sr_to_concept(c1, c2, temp_concepts, SR)\n",
    "                \n",
    "        return result_dict, temp_concepts\n",
    "\n",
    "    def add_concept_concept_edges(self):\n",
    "        cc_dict, SR_count_dict = self.get_concept_concept_edge_values()\n",
    "        for c1, c2 in cc_dict:\n",
    "            SR_c1_c2 = cc_dict[(c1,c2)]\n",
    "            \n",
    "            self.cc_edges.append([c1, c2, SR_c1_c2/SR_count_dict[c1]])\n",
    "            self.cc_edges.append([c2, c1, SR_c1_c2/SR_count_dict[c2]])\n",
    "\n",
    "    def output_writer(self, file_name):\n",
    "        with open(file_name, 'w', newline='\\n') as write_obj:\n",
    "            csv_writer = writer(write_obj)\n",
    "            for data in self.cc_edges:\n",
    "                csv_writer.writerow(data) \n",
    "        return self.cc_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_concepts(filename):\n",
    "    concept_ids = set()\n",
    "    with open(filename, newline='\\n') as csvfile:\n",
    "        red = reader(csvfile, delimiter=CORPUS_DELIMITER)\n",
    "        for row in red:\n",
    "            concept_ids.add(row[1])\n",
    "    return concept_ids\n",
    "\n",
    "try:\n",
    "    wiki_bipartite_edges\n",
    "except NameError:\n",
    "    candidate_concepts = get_candidate_concepts(wiki_files[\"wiki_bipartite_edges\"])\n",
    "else:\n",
    "    candidate_concepts = set( [c for a,c,v in wiki_bipartite_edges ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_edge_object = ConceptEdgeGenerator(corpus_files[\"concept-concepts\"], candidate_concepts)\n",
    "concept_edge_object.add_concept_concept_edges()\n",
    "wiki_concept_edges = concept_edge_object.output_writer(wiki_files[\"wiki_concept_edges\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_concept_edges[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagerank Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the Pagerank for the generated graph, we are using <a href = \"https://graph-tool.skewed.de/static/doc/quickstart.html\" >Graph_tool </a> instead of NetworkX library as it is extremely faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Import Libraries </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:27:13.280328Z",
     "start_time": "2020-04-20T18:27:13.110909Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import graph_tool as gt\n",
    "from graph_tool import centrality as ct\n",
    "from csv import reader, writer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \n",
    "    def __init__(self, bipartite_edges, concept_edges):\n",
    "        \n",
    "        self.graph = gt.Graph()\n",
    "        self.pagerank = dict()\n",
    "        self.anchor_dictionary = dict()\n",
    "        self.massive_dict = dict()\n",
    "        self.counter = 0\n",
    "        edges, weights = self.get_edges_weights(bipartite_edges, bipartite_edges)\n",
    "        self.edges = np.array(edges)\n",
    "        self.weights = np.array(weights)\n",
    "        \n",
    "        self.graph.add_edge_list(self.edges, hashed=True, string_vals=True)\n",
    "        self.ew = self.graph.new_edge_property(\"double\")\n",
    "        self.ew.a = self.weights \n",
    "        self.graph.ep['edge_weight'] = self.ew\n",
    "        self.pr_result = list()\n",
    "        \n",
    "    def calculate_pageranks(self):\n",
    "        pagerank_dict = ct.pagerank(self.graph, weight = self.ew)\n",
    "        result_dict = dict()\n",
    "\n",
    "        for anchor in self.anchor_dictionary:\n",
    "            result_dict[anchor] = [(concept, pagerank_dict[self.massive_dict[concept]]) for concept in self.anchor_dictionary[anchor]]\n",
    "            result_dict[anchor].sort(key = lambda x: x[1], reverse = True)\n",
    "        self.pagerank = result_dict\n",
    "        self.pr_result = [(anchor, concept, score) for anchor in self.pagerank for concept, score in self.pagerank[anchor]]\n",
    "    \n",
    "    def get_anchor_dictionary(self, ac_edges):\n",
    "        for anchor, concept in ac_edges:\n",
    "            if anchor not in self.anchor_dictionary:\n",
    "                self.anchor_dictionary[anchor] = [concept]\n",
    "            else:\n",
    "                self.anchor_dictionary[anchor].append(concept)\n",
    "        \n",
    "    def edge_indexer(self, data):\n",
    "        edges = list()\n",
    "        weights = list()\n",
    "        for a,b,c in data:\n",
    "            for item in [a, b]:\n",
    "                if item not in self.massive_dict:\n",
    "                    self.massive_dict[item] = self.counter\n",
    "                    self.counter += 1\n",
    "            edges.append([a,b])\n",
    "            weights.append(c)\n",
    "        return edges, weights\n",
    "    \n",
    "    def get_edges_weights(self, bipartite_edges, concept_edges):\n",
    "        ac_edges, ac_weights = self.edge_indexer(bipartite_edges)\n",
    "        self.get_anchor_dictionary(ac_edges)\n",
    "        cc_edges, cc_weights = self.edge_indexer(concept_edges)\n",
    "        return ac_edges+cc_edges, ac_weights+cc_weights\n",
    "    \n",
    "    def output_writer(self, file_name):\n",
    "        with open(file_name, 'w', newline='\\n') as write_obj:\n",
    "            csv_writer = writer(write_obj)\n",
    "            for data in self.pr_result:\n",
    "                csv_writer.writerow(data)\n",
    "        return self.pr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_data(filename):\n",
    "    edges = list()\n",
    "    with open(filename, newline='\\n') as csvfile:\n",
    "        red = reader(csvfile, delimiter=CORPUS_DELIMITER)\n",
    "        for a,b,c in red:\n",
    "            edges.append([a, b, float(c)])\n",
    "    return edges\n",
    "\n",
    "try:\n",
    "    wiki_bipartite_edges\n",
    "except:\n",
    "    wiki_bipartite_edges = get_edges_data(wiki_files[\"wiki_bipartite_edges\"])\n",
    "    \n",
    "try:\n",
    "    wiki_concept_edges\n",
    "except:\n",
    "    wiki_concept_edges = get_edges_data(wiki_files[\"wiki_concept_edges\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_object = Graph(wiki_bipartite_edges, wiki_concept_edges)\n",
    "graph_object.calculate_pageranks()\n",
    "\n",
    "wiki_pagerank = graph_object.output_writer(wiki_files[\"wiki_pagerank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a8739039', 'c1657702', 0.0009064184202898014)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_pagerank[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Output Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Import Libraries </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T18:27:13.284951Z",
     "start_time": "2020-04-20T18:27:13.281460Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from csv import reader, writer\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readable_output(anchor_dict, concept_dict, pagerank_dict):\n",
    "    result = list()\n",
    "    for anchor in pagerank_dict:\n",
    "        for concept in pagerank_dict[anchor]:\n",
    "            result.append([anchor_dict[anchor], concept_dict[concept]])\n",
    "    return result\n",
    "\n",
    "def final_output_writer(file_name, items):\n",
    "    with open(file_name, 'w', newline='\\n') as write_obj:\n",
    "        csv_writer = writer(write_obj)\n",
    "        for data in items:\n",
    "            csv_writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_reader(filename):\n",
    "    result = dict()\n",
    "    with open(filename, newline='\\n') as csvfile:\n",
    "        red = reader(csvfile, delimiter=CORPUS_DELIMITER)\n",
    "        for a,b in red:\n",
    "            result[a] = b\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_reader(filename, CONCEPT_LIMIT):\n",
    "    result = dict()\n",
    "    with open(filename, newline='\\n') as csvfile:\n",
    "        red = reader(csvfile, delimiter=',')\n",
    "        for row in red:\n",
    "            if row[0] not in result:\n",
    "                result[row[0]] = [row[1]]\n",
    "            else:\n",
    "                if len(result[row[0]]) <= CONCEPT_LIMIT:\n",
    "                    result[row[0]].append(row[1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCEPT_LIMIT = 5\n",
    "\n",
    "anchor_dict = id_reader(corpus_files[\"anchor-id\"])\n",
    "concept_dict = id_reader(corpus_files[\"concept-id\"])\n",
    "\n",
    "pagerank_dict = pagerank_reader(wiki_files[\"wiki_pagerank\"], CONCEPT_LIMIT)\n",
    "wiki_output = get_readable_output(anchor_dict, concept_dict, pagerank_dict)\n",
    "final_output_writer(wiki_files[\"wiki_output\"], wiki_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['English', 'https://en.wikipedia.org/wiki/English%20language'],\n",
       " ['English', 'https://en.wikipedia.org/wiki/England'],\n",
       " ['English', 'https://en.wikipedia.org/wiki/English%20people'],\n",
       " ['English', 'https://en.wikipedia.org/wiki/English%20studies'],\n",
       " ['English', 'https://en.wikipedia.org/wiki/Kingdom%20of%20England'],\n",
       " ['English', 'https://en.wikipedia.org/wiki/English%20American'],\n",
       " ['Mercury', 'https://en.wikipedia.org/wiki/Mercury%20%28planet%29'],\n",
       " ['Mercury', 'https://en.wikipedia.org/wiki/Mercury%20%28mythology%29'],\n",
       " ['Mercury', 'https://en.wikipedia.org/wiki/Mercury%20Records'],\n",
       " ['Mercury', 'https://en.wikipedia.org/wiki/Mercury%20%28automobile%29'],\n",
       " ['Mercury', 'https://en.wikipedia.org/wiki/Project%20Mercury'],\n",
       " ['Mercury', 'https://en.wikipedia.org/wiki/Mercury%20%28element%29'],\n",
       " ['Mars', 'https://en.wikipedia.org/wiki/Mars'],\n",
       " ['Mars', 'https://en.wikipedia.org/wiki/Mars%20%28mythology%29'],\n",
       " ['Mars', 'https://en.wikipedia.org/wiki/Mars%2C%20Incorporated'],\n",
       " ['Mars', 'https://en.wikipedia.org/wiki/Mars%20%28god%29'],\n",
       " ['Mars', 'https://en.wikipedia.org/wiki/Mars%20%28astrology%29'],\n",
       " ['Mars', 'https://en.wikipedia.org/wiki/Mars%20%28band%29'],\n",
       " ['war', 'https://en.wikipedia.org/wiki/war'],\n",
       " ['war', 'https://en.wikipedia.org/wiki/World%20War%20II'],\n",
       " ['war', 'https://en.wikipedia.org/wiki/Second%20World%20War'],\n",
       " ['war', 'https://en.wikipedia.org/wiki/First%20World%20War'],\n",
       " ['war', 'https://en.wikipedia.org/wiki/World%20War%20I'],\n",
       " ['war', 'https://en.wikipedia.org/wiki/war%20film'],\n",
       " ['Sun', 'https://en.wikipedia.org/wiki/Sun'],\n",
       " ['Sun', 'https://en.wikipedia.org/wiki/Sun%20Microsystems'],\n",
       " ['Sun', 'https://en.wikipedia.org/wiki/The%20Sun%20%28United%20Kingdom%29'],\n",
       " ['Sun', 'https://en.wikipedia.org/wiki/Sun%20%28astrology%29'],\n",
       " ['Sun', 'https://en.wikipedia.org/wiki/Sun%20Records'],\n",
       " ['Sun', 'https://en.wikipedia.org/wiki/Sun-Hwa%20Kwon'],\n",
       " ['Earth', 'https://en.wikipedia.org/wiki/Earth'],\n",
       " ['Earth', 'https://en.wikipedia.org/wiki/Earth%20%28American%20band%29'],\n",
       " ['Earth', 'https://en.wikipedia.org/wiki/Earth%20%28classical%20element%29'],\n",
       " ['Earth', 'https://en.wikipedia.org/wiki/Earth%20%281998%20film%29'],\n",
       " ['Earth', 'https://en.wikipedia.org/wiki/Earth%20%28planet%29'],\n",
       " ['Earth', 'https://en.wikipedia.org/wiki/Earth%20%28Noon%20Universe%29'],\n",
       " ['name', 'https://en.wikipedia.org/wiki/name'],\n",
       " ['name', 'https://en.wikipedia.org/wiki/Botanical%20name'],\n",
       " ['name', 'https://en.wikipedia.org/wiki/Welsh%20placenames'],\n",
       " ['name', 'https://en.wikipedia.org/wiki/given%20name'],\n",
       " ['name',\n",
       "  'https://en.wikipedia.org/wiki/List%20of%20Ohio%20county%20name%20etymologies'],\n",
       " ['name', 'https://en.wikipedia.org/wiki/ring%20name'],\n",
       " ['deserts', 'https://en.wikipedia.org/wiki/deserts'],\n",
       " ['deserts', 'https://en.wikipedia.org/wiki/Desert'],\n",
       " ['deserts', 'https://en.wikipedia.org/wiki/desert'],\n",
       " ['deserts', 'https://en.wikipedia.org/wiki/Deserts%20of%20Australia'],\n",
       " ['deserts', 'https://en.wikipedia.org/wiki/desertion'],\n",
       " ['deserts', 'https://en.wikipedia.org/wiki/desert%20fungi'],\n",
       " ['Solar System', 'https://en.wikipedia.org/wiki/Solar%20System'],\n",
       " ['Solar System', 'https://en.wikipedia.org/wiki/Solar%20system'],\n",
       " ['Solar System', 'https://en.wikipedia.org/wiki/Solar%20System%20%28song%29'],\n",
       " ['Solar System',\n",
       "  'https://en.wikipedia.org/wiki/Leigh%20Brackett%20Solar%20System'],\n",
       " ['Solar System',\n",
       "  'https://en.wikipedia.org/wiki/Solar%20System%20in%20fiction'],\n",
       " ['Solar System',\n",
       "  'https://en.wikipedia.org/wiki/age%20of%20the%20Solar%20System'],\n",
       " ['Moon', 'https://en.wikipedia.org/wiki/Moon'],\n",
       " ['Moon', 'https://en.wikipedia.org/wiki/Moon%20%28film%29'],\n",
       " ['Moon', 'https://en.wikipedia.org/wiki/Moon%20%28astrology%29'],\n",
       " ['Moon', 'https://en.wikipedia.org/wiki/Muhu'],\n",
       " ['Moon', 'https://en.wikipedia.org/wiki/Moon%20%28Middle-earth%29'],\n",
       " ['Moon', 'https://en.wikipedia.org/wiki/Moon%20%28visual%20novel%29'],\n",
       " ['Roman god', 'https://en.wikipedia.org/wiki/Roman%20mythology'],\n",
       " ['Roman god', 'https://en.wikipedia.org/wiki/Roman%20god'],\n",
       " ['Roman god', 'https://en.wikipedia.org/wiki/Mercury%20%28mythology%29'],\n",
       " ['Roman god', 'https://en.wikipedia.org/wiki/List%20of%20Roman%20deities'],\n",
       " ['Roman god', 'https://en.wikipedia.org/wiki/Janus'],\n",
       " ['Roman god', 'https://en.wikipedia.org/wiki/Ancient%20Roman%20religion'],\n",
       " ['terrestrial planet', 'https://en.wikipedia.org/wiki/terrestrial%20planet'],\n",
       " ['terrestrial planet', 'https://en.wikipedia.org/wiki/Rocky%20planet'],\n",
       " ['naked eye', 'https://en.wikipedia.org/wiki/naked%20eye'],\n",
       " ['naked eye',\n",
       "  'https://en.wikipedia.org/wiki/Naked%20eye%23Naked%20eye%20in%20astronomy'],\n",
       " ['naked eye', 'https://en.wikipedia.org/wiki/classical%20planet'],\n",
       " ['naked eye', 'https://en.wikipedia.org/wiki/Bright%20Star%20Catalogue'],\n",
       " ['naked eye', 'https://en.wikipedia.org/wiki/Naked-eye%20stars'],\n",
       " ['naked eye', 'https://en.wikipedia.org/wiki/naked-eye%20stars'],\n",
       " ['impact craters', 'https://en.wikipedia.org/wiki/impact%20craters'],\n",
       " ['impact craters', 'https://en.wikipedia.org/wiki/impact%20crater'],\n",
       " ['impact craters', 'https://en.wikipedia.org/wiki/Impact%20crater'],\n",
       " ['impact craters',\n",
       "  'https://en.wikipedia.org/wiki/list%20of%20impact%20craters%20on%20Earth'],\n",
       " ['effect', 'https://en.wikipedia.org/wiki/Causality'],\n",
       " ['effect', 'https://en.wikipedia.org/wiki/therapeutic%20effect'],\n",
       " ['effect', 'https://en.wikipedia.org/wiki/Effects%20unit'],\n",
       " ['effect', 'https://en.wikipedia.org/wiki/audio%20signal%20processing'],\n",
       " ['effect', 'https://en.wikipedia.org/wiki/Special%20effect'],\n",
       " ['effect', 'https://en.wikipedia.org/wiki/Sound%20effect'],\n",
       " ['valleys', 'https://en.wikipedia.org/wiki/valleys'],\n",
       " ['valleys', 'https://en.wikipedia.org/wiki/South%20Wales%20Valleys'],\n",
       " ['valleys', 'https://en.wikipedia.org/wiki/Valley'],\n",
       " ['valleys', 'https://en.wikipedia.org/wiki/Wadi'],\n",
       " ['valleys', 'https://en.wikipedia.org/wiki/valleys%20of%20the%20Alps'],\n",
       " ['valleys', 'https://en.wikipedia.org/wiki/Valleys%20of%20South%20Tyrol'],\n",
       " ['reddish appearance',\n",
       "  'https://en.wikipedia.org/wiki/Mars%20surface%20color'],\n",
       " ['polar ice caps', 'https://en.wikipedia.org/wiki/polar%20ice%20caps'],\n",
       " ['polar ice caps', 'https://en.wikipedia.org/wiki/Arctic%20ice%20pack'],\n",
       " ['polar ice caps', 'https://en.wikipedia.org/wiki/Polar%20ice%20packs'],\n",
       " ['polar ice caps', 'https://en.wikipedia.org/wiki/Antarctic%20ice%20sheet'],\n",
       " ['astronomical bodies',\n",
       "  'https://en.wikipedia.org/wiki/astronomical%20bodies'],\n",
       " ['astronomical bodies',\n",
       "  'https://en.wikipedia.org/wiki/astronomical%20object'],\n",
       " ['astronomical bodies',\n",
       "  'https://en.wikipedia.org/wiki/Astronomical%20object'],\n",
       " ['astronomical bodies', 'https://en.wikipedia.org/wiki/astrolatry'],\n",
       " ['surface features',\n",
       "  'https://en.wikipedia.org/wiki/Planetary%20nomenclature']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "260px",
    "width": "412px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
